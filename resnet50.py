# -*- coding: utf-8 -*-
"""Resnet50.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mJpo7kJjAnTUz7fq0hh71U7dJvWw0BmZ
"""

import zipfile
zip_file = "/content/archive (22) (1).zip"

with zipfile.ZipFile(zip_file, 'r') as zip_ref:
    zip_ref.extractall("/content/dataset")  # Extract to a folder inside Colab

print("Dataset extracted successfully!")

from google.colab import files
uploaded = files.upload()

import cv2
 # Import cv2_imshow from google.colab.patches
from google.colab.patches import cv2_imshow

# Path to the correct image file (update with actual path from previous steps)
image_path = "/content/dataset/Celebrity Faces Dataset/Hugh Jackman/049_beaf3777.jpg"  # Adjust if necessary

# Read the image
img = cv2.imread(image_path)

cv2_imshow(img) # Use the imported cv2_imshow function to display the image

import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from collections import Counter
import tensorflow as tf
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

dataset_path = "/content/dataset/Celebrity Faces Dataset"
X = []
y = []

for celebrity in os.listdir(dataset_path):
    celebrity_folder = os.path.join(dataset_path, celebrity)
    if os.path.isdir(celebrity_folder):
        for image_file in os.listdir(celebrity_folder):
            img_path = os.path.join(celebrity_folder, image_file)
            img = cv2.imread(img_path)
            if img is not None:
                img = cv2.resize(img, (224, 224))
                X.append(img)
                y.append(celebrity)

X = np.array(X)
y = np.array(y)

print("Total samples:", len(X))
print("Classes:", set(y))

le = LabelEncoder()
y_encoded = le.fit_transform(y)

# =======================
# 3. Split & Preprocess Data
# =======================

X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42
) #stratify is used to balance all the data between train test split

# Apply ResNet50-style preprocessing
X_train = preprocess_input(X_train)
X_test = preprocess_input(X_test)

from collections import Counter
print(Counter(y))  # your raw label list before encoding

target_classes_to_boost = [
    'Angelina Jolie', 'Natalie Portman', 'Sandra Bullock',
    'Denzel Washington', 'Will Smith', 'Megan Fox',
    'Johnny Depp', 'Kate Winslet', 'Brad Pitt'
]

from sklearn.utils.class_weight import compute_class_weight
import numpy as np

boosted_weight = 2.0
normal_weight = 1.0

class_weights = {}

for i, class_name in enumerate(le.classes_):
    if class_name in target_classes_to_boost:
        class_weights[i] = boosted_weight
    else:
        class_weights[i] = normal_weight

print("Custom class weights:", class_weights)

image_gen = ImageDataGenerator(
    rotation_range=20,  # Increased rotation range
    width_shift_range=0.2,  # Increased width shift range
    height_shift_range=0.2,  # Increased height shift range
    shear_range=0.2,  # Increased shear range
    zoom_range=0.2,  # Increased zoom range
    horizontal_flip=True,
    fill_mode='nearest'
)

base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

for layer in base_model.layers[:-30]:
    layer.trainable = False

model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(17, activation='softmax')  # 17 classes
])

# Compile the model before freezing layers
model.compile(
    optimizer=Adam(learning_rate=1e-5),  # You can experiment with different learning rates
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Freeze base layers (you can experiment with unfreezing more layers)

# Rest of your training code...

model.save('best_model.keras')

from google.colab import files
files.download("best_model.keras")

import os
print(os.listdir())

test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc * 100:.2f}%")

y_pred = model.predict(X_test)
y_pred_labels = np.argmax(y_pred, axis = 1)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, y_pred_labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)
disp.plot(xticks_rotation='vertical', cmap='Blues')
plt.title("Confusion Matrix")
plt.show()

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred_labels, target_names=le.classes_))
#precision is out of all the times model said yes how many were actually correct and recall is how many times the model guessed on true images correctly

from google.colab import files
uploaded = files.upload()

import os
filename = "kate.jpg"
print("uploaded:", filename)



import cv2
import matplotlib.pyplot as plt
img = cv2.imread(filename)
img_resized = cv2.resize(img, (244,244))
plt.imshow(img_resized)
plt.axis('off')
plt.title("uploaded image")
plt.show()

from tensorflow.keras.applications.resnet50 import preprocess_input
import numpy as np
input_img = np.expand_dims(img_resized, axis = 0)
input_img = preprocess_input(input_img)

prediction = model.predict(input_img)
class_index = np.argmax(prediction)
confidence = prediction[0][class_index]
predicted_name = le.classes_[class_index]

print(f"Predicted Celebrity: {predicted_name} ({confidence*100:.2f}% confidence)")

if confidence < 0.50:
    print("⚠️ This might not be one of the known 17 celebrities.")

